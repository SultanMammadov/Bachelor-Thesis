{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46be54ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    StratifiedKFold,\n",
    "    RandomizedSearchCV\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    make_scorer\n",
    ")\n",
    "\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ead220-49a2-4456-8bf2-5110759288d6",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee07eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'telecom_data.csv')\n",
    "\n",
    "data.drop(['CustomerID'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7ebf5-70b5-448f-8fb6-0fad3b4e7f0b",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc1a539d-664d-46a5-a4cb-25ca366fe279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Monthly Engagement Ratio\n",
    "data['MonthlyEngagementRatio'] = data['ViewingHoursPerWeek'] / data['MonthlyCharges']\n",
    "\n",
    "# 2. Loyalty Indicator\n",
    "data['LoyaltyIndicator'] = pd.cut(data['AccountAge'], \n",
    "                                   bins=[0, 12, 36, float('inf')], \n",
    "                                   labels=['New', 'Mid-Level', 'Loyal'])\n",
    "\n",
    "# 3. Content Diversity\n",
    "data['ContentDiversity'] = (data['ContentType'] == 'Both').astype(int)\n",
    "\n",
    "# 4. Premium Cost Index\n",
    "average_cost = data.groupby('SubscriptionType')['MonthlyCharges'].transform('mean')\n",
    "data['PremiumCostIndex'] = data['MonthlyCharges'] > average_cost\n",
    "\n",
    "# 5. Device Engagement\n",
    "data['DeviceEngagement'] = ((data['DeviceRegistered'] != 'Unknown') & (data['MultiDeviceAccess'] == 'Yes')).astype(int)\n",
    "\n",
    "# 6. Support Need Intensity\n",
    "data['SupportNeedIntensity'] = data['SupportTicketsPerMonth'] / (data['AccountAge'] + 1)  # Avoid division by zero\n",
    "\n",
    "# 7. Churn Risk Indicators\n",
    "data['HighWatchlist'] = (data['WatchlistSize'] > 20).astype(int)\n",
    "\n",
    "# 8. FrequentDownloader\n",
    "data['FrequentDownloader'] = (data['ContentDownloadsPerMonth'] > data['ContentDownloadsPerMonth'].mean()).astype(int)\n",
    "\n",
    "# 9. Payment Stability\n",
    "stable_payment_methods = ['Bank transfer', 'Credit card']\n",
    "data['PaymentStability'] = data['PaymentMethod'].isin(stable_payment_methods).astype(int)\n",
    "\n",
    "# 10. Parental Control Impact\n",
    "data['ParentalControlImpact'] = ((data['ParentalControl'] == 'Yes') & (data['SubtitlesEnabled'] == 'Yes')).astype(int)\n",
    "\n",
    "# 11. Feature Interactions\n",
    "data['SubscriptionContentInteraction'] = data['SubscriptionType'] + \"_\" + data['ContentType']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502fd31-284c-4b24-92aa-17448cbb509c",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c9dbdb1-ee06-4ef7-97b3-26aa7a2a2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "data['SubscriptionType'] = le.fit_transform(data['SubscriptionType'])\n",
    "data['PaymentMethod'] = le.fit_transform(data['PaymentMethod'])\n",
    "data['PaperlessBilling'] = le.fit_transform(data['PaperlessBilling'])\n",
    "data['ContentType'] = le.fit_transform(data['ContentType'])\n",
    "data['MultiDeviceAccess'] = le.fit_transform(data['MultiDeviceAccess'])\n",
    "data['DeviceRegistered'] = le.fit_transform(data['DeviceRegistered'])\n",
    "data['GenrePreference'] = le.fit_transform(data['GenrePreference'])\n",
    "data['Gender'] = le.fit_transform(data['Gender'])\n",
    "data['ParentalControl'] = le.fit_transform(data['ParentalControl'])\n",
    "data['SubtitlesEnabled'] = le.fit_transform(data['SubtitlesEnabled'])\n",
    "data['LoyaltyIndicator'] = le.fit_transform(data['LoyaltyIndicator'])\n",
    "data['PremiumCostIndex'] = le.fit_transform(data['PremiumCostIndex'])\n",
    "data['SubscriptionContentInteraction'] = le.fit_transform(data['SubscriptionContentInteraction'])\n",
    "\n",
    "\n",
    "data.drop(['TotalCharges', 'ContentType', 'DeviceEngagement', 'SubscriptionContentInteraction'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaab2b0-0ec4-4705-9a30-a7a0eb5b07f5",
   "metadata": {},
   "source": [
    "# DATA SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5098e5a1-8c14-44b2-a08a-ab582c415830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train_val count: 195001\n",
      "data_test count: 48786\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22016\\3433793710.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Apply SMOTE + ENN to the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0msmoteenn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mX_train_resampled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_resampled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmoteenn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         y_ = (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\combine\\_smote_enn.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mX_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_res\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmote_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menn_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         y_ = (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\under_sampling\\_prototype_selection\\_edited_nearest_neighbours.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[0mX_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_class_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[0my_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_class_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0mnnhood_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m                 \u001b[0mnnhood_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnnhood_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind_sel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"mode\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    750\u001b[0m                 \u001b[0mkwds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meffective_metric_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m             chunked_results = list(\n\u001b[0m\u001b[0;32m    753\u001b[0m                 pairwise_distances_chunked(\n\u001b[0;32m    754\u001b[0m                     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   1715\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1716\u001b[0m             \u001b[0mX_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1717\u001b[1;33m         \u001b[0mD_chunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1718\u001b[0m         if (X is Y or Y is None) and PAIRWISE_DISTANCE_FUNCTIONS.get(\n\u001b[0;32m   1719\u001b[0m             \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   1887\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1889\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    328\u001b[0m             )\n\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_euclidean_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_norm_squared\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_norm_squared\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[1;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_train_val = data[data['AccountAge']>24]\n",
    "data_test = data[data['AccountAge'] <=24]\n",
    "\n",
    "print('data_train_val count:', data_train_val['Churn'].count())\n",
    "print('data_test count:', data_test['Churn'].count())\n",
    "\n",
    "# Assume 'Target' is the name of the target column\n",
    "X = data_train_val.drop('Churn', axis=1)\n",
    "y = data_train_val['Churn']\n",
    "\n",
    "# Split the dataset into training (75%) and validation (25%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE + ENN to the training data\n",
    "smoteenn = SMOTEENN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smoteenn.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0ca9d-6251-4f1b-befd-5efee4401508",
   "metadata": {},
   "source": [
    "# Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3de3e7-df45-4965-ab0d-eae6c13e7f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the resampled training data and the validation data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)  \n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2a109-b2fb-42d3-b596-8470d21cd918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aff8d82",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293df1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model with higher max_iter\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Define the refined random search hyperparameter space\n",
    "param_distributions = [\n",
    "    {\n",
    "        'solver': ['liblinear'],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    {\n",
    "        'solver': ['lbfgs', 'newton-cg'],\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    {\n",
    "        'solver': ['saga'],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': ['balanced'],\n",
    "        'l1_ratio': [0.1, 0.5, 0.7, 1.0]  # Only for elasticnet\n",
    "    }\n",
    "]\n",
    "\n",
    "# Replace CV with Stratified Cross-Validation\n",
    "stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) \n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    log_reg,\n",
    "    param_distributions,\n",
    "    scoring=make_scorer(f1_score, average='binary'), \n",
    "    cv=stratified_cv,  \n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    n_iter=10, \n",
    "    random_state=42,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# Measure training start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model on the resampled and scaled training data\n",
    "random_search.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Measure end time for training\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best parameters and retrain the model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = best_model.predict(X_val_scaled)\n",
    "y_val_pred_proba = best_model.predict_proba(X_val_scaled)[:, 1]  # For log loss and ROC-AUC\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_val, y_val_pred),\n",
    "    \"Precision\": precision_score(y_val, y_val_pred, average='binary'),  \n",
    "    \"Recall\": recall_score(y_val, y_val_pred, average='binary'),        \n",
    "    \"F1-Score\": f1_score(y_val, y_val_pred, average='binary'),          \n",
    "    \"Log Loss\": log_loss(y_val, y_val_pred_proba),\n",
    "    \"Matthews Correlation Coefficient\": matthews_corrcoef(y_val, y_val_pred),\n",
    "    \"ROC-AUC\": roc_auc_score(y_val, y_val_pred_proba),\n",
    "}\n",
    "\n",
    "# Print the evaluation metrics for validation set\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report for validation set\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and print training time\n",
    "train_time = end_time - start_time\n",
    "print(\"\\nTraining Time:\", round(train_time, 2), \"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496cd818-edb1-4b2c-8857-f4fa52bbbfeb",
   "metadata": {},
   "source": [
    "# Feature Importance Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b83314-1cf8-4b8f-923d-23da332bd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature importance for Logistic Regression\n",
    "feature_importance = np.abs(log_reg.coef_[0])  # Absolute values of coefficients\n",
    "feature_names = X.columns  # Feature names\n",
    "importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": feature_importance})\n",
    "\n",
    "#  Convert importance values to percentages\n",
    "importance_df[\"Importance (%)\"] = (importance_df[\"Importance\"] / importance_df[\"Importance\"].sum()) * 100\n",
    "\n",
    "#  Sort features by importance in descending order\n",
    "importance_df = importance_df.sort_values(by=\"Importance (%)\", ascending=False)\n",
    "\n",
    "#  Plot top feature importances with color gradient\n",
    "top_features = importance_df.head(15)\n",
    "colors = plt.cm.Blues(np.linspace(1, 0.4, len(top_features)))\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Increased the figure size\n",
    "bars = plt.barh(\n",
    "    top_features[\"Feature\"],\n",
    "    top_features[\"Importance (%)\"],\n",
    "    color=colors,\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.9,\n",
    ")\n",
    "\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.title(\"Feature Importances (Logistic Regression)\", fontsize=18)\n",
    "plt.xlabel(\"Importance (%)\", fontsize=14)\n",
    "\n",
    "# Add importance values on bars (rounded to 2 decimal places)\n",
    "for bar, value in zip(bars, top_features[\"Importance (%)\"]):\n",
    "    plt.text(\n",
    "        bar.get_width() + 0.001,  # Adjusted to provide more spacing\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{value:.1f}%\",  # Rounded to 2 decimal places\n",
    "        va=\"center\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c980cdd-0b0c-47fd-967d-e0846df81343",
   "metadata": {},
   "source": [
    "# Coefficients - Feature Importance Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d861b1e3-96de-4e89-b68c-e58d4ebb2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'log_reg' is your trained Logistic Regression model and 'X' is your feature DataFrame\n",
    "\n",
    "# Extract coefficients\n",
    "coefficients = log_reg.coef_[0]  # For binary classification, take the first row\n",
    "absolute_coefficients = np.abs(coefficients)  # Use absolute values for importance ranking\n",
    "\n",
    "# Pair coefficients with feature names\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": absolute_coefficients\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# (Optional) Convert to percentages\n",
    "feature_importance_df[\"Importance (%)\"] = (feature_importance_df[\"Importance\"] / feature_importance_df[\"Importance\"].sum()) * 100\n",
    "\n",
    "# Display the DataFrame with feature importance values\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df3161-3877-4cb0-9a10-03e6d09194f0",
   "metadata": {},
   "source": [
    "# Permutation Importance - Feature Importance Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6d2f4-8705-44a8-8914-9e848830ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate Permutation Importance on the validation set using F1 score\n",
    "perm_importance = permutation_importance(\n",
    "    log_reg,  # Trained logistic regression model\n",
    "    X_val_scaled,  # Scaled validation features\n",
    "    y_val,  # Validation target labels\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    scoring=make_scorer(f1_score, average=\"binary\")  # Using F1 score as the scoring metric\n",
    ")\n",
    "\n",
    "# Extract feature importances and feature names\n",
    "feature_importances = perm_importance.importances_mean\n",
    "feature_names = X.columns\n",
    "\n",
    "# Convert importance values to percentages\n",
    "total_importance = feature_importances.sum()\n",
    "feature_importance_percentages = (feature_importances / total_importance) * 100\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance (%)\": feature_importance_percentages\n",
    "}).sort_values(by=\"Importance (%)\", ascending=False)\n",
    "\n",
    "# Display the feature importance DataFrame\n",
    "print(\"\\nPermutation Importance for Features (as Percentage):\")\n",
    "print(perm_importance_df)\n",
    "\n",
    "# Optionally save the results to an Excel file\n",
    "output_file = r\"permutation_importance_f1_score.xlsx\"\n",
    "perm_importance_df.to_excel(output_file, index=False)\n",
    "print(f\"\\nPermutation importance percentages have been saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4231f3b0-197a-4dd8-9f99-64e5ac77cd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e770ff96",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9148b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,  # To suppress warning in newer versions\n",
    "    eval_metric='logloss',  # Required by XGBoost\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define the hyperparameter space for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 150], \n",
    "    'learning_rate': [0.05, 0.1], \n",
    "    'max_depth': [4, 8],  \n",
    "    'min_child_weight': [1, 3],  \n",
    "    'subsample': [0.8], \n",
    "    'colsample_bytree': [0.8],  \n",
    "    'gamma': [0],  \n",
    "    'reg_alpha': [0], \n",
    "    'reg_lambda': [1]  \n",
    "}\n",
    "\n",
    "\n",
    "# Define RandomizedSearchCV with F1-score as the scoring metric and StratifiedKFold for cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model,\n",
    "    param_distributions,\n",
    "    scoring=make_scorer(f1_score, average='binary'),  # Weighted F1-score for imbalanced datasets\n",
    "    cv=stratified_kfold,  # Use StratifiedKFold for cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    n_iter=10,  # Number of parameter settings sampled\n",
    "    random_state=42,\n",
    "    error_score='raise'  # Set error_score to raise for debugging\n",
    ")\n",
    "\n",
    "# Measure training start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model on the resampled and scaled training data\n",
    "random_search.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Measure end time for training\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best parameters and retrain the model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = best_model.predict(X_val_scaled)\n",
    "y_val_pred_proba = best_model.predict_proba(X_val_scaled)[:, 1]  # For log loss and ROC-AUC\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_val, y_val_pred),\n",
    "    \"Precision\": precision_score(y_val, y_val_pred, average='binary'),  # Added average\n",
    "    \"Recall\": recall_score(y_val, y_val_pred, average='binary'),        # Added average\n",
    "    \"F1-Score\": f1_score(y_val, y_val_pred, average='binary'),          # Added average\n",
    "    \"Log Loss\": log_loss(y_val, y_val_pred_proba),                        # No average needed\n",
    "    \"Matthews Correlation Coefficient\": matthews_corrcoef(y_val, y_val_pred),  # No average needed\n",
    "    \"ROC-AUC\": roc_auc_score(y_val, y_val_pred_proba),                    # No average needed\n",
    "}\n",
    "\n",
    "# Print the evaluation metrics for validation set\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report for validation set\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and print training time\n",
    "train_time = end_time - start_time\n",
    "print(\"\\nTraining Time:\", round(train_time, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b5059-fee6-497a-9844-0b9ec59323db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e0901df",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter space for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2'],  # Fixed deprecated 'auto'\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# Define RandomizedSearchCV with F1-score as the scoring metric\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_model,\n",
    "    param_distributions,\n",
    "    scoring=make_scorer(f1_score, average=\"binary\"),  # Weighted F1-score for imbalanced datasets\n",
    "    cv=stratified_kfold,  # Use StratifiedKFold for cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    n_iter=10,  # Number of parameter settings sampled\n",
    "    random_state=42,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# Measure training start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model on the resampled and scaled training data\n",
    "random_search.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Measure end time for training\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best parameters and retrain the model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = best_model.predict(X_val_scaled)\n",
    "y_val_pred_proba = best_model.predict_proba(X_val_scaled)[:, 1]  # For log loss and ROC-AUC\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_val, y_val_pred),\n",
    "    \"Precision\": precision_score(y_val, y_val_pred, average=\"binary\"),\n",
    "    \"Recall\": recall_score(y_val, y_val_pred, average=\"binary\"),\n",
    "    \"F1-Score\": f1_score(y_val, y_val_pred, average=\"binary\"),\n",
    "    \"Log Loss\": log_loss(y_val, y_val_pred_proba),\n",
    "    \"Matthews Correlation Coefficient\": matthews_corrcoef(y_val, y_val_pred),\n",
    "    \"ROC-AUC\": roc_auc_score(y_val, y_val_pred_proba)  # Fixed for binary classification\n",
    "}\n",
    "\n",
    "# Print the evaluation metrics for validation set\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report for validation set\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and print training time\n",
    "train_time = end_time - start_time\n",
    "print(\"\\nTraining Time:\", round(train_time, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49948fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30ab08b2",
   "metadata": {},
   "source": [
    "# LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5bd163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LGBM classifier\n",
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter space for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [10, 20], \n",
    "    'num_leaves': [31, 40], \n",
    "    'min_child_samples': [10], \n",
    "    'subsample': [0.8, 1.0],  \n",
    "    'colsample_bytree': [0.8, 1.0], \n",
    "    'reg_alpha': [0, 0.1],  \n",
    "    'reg_lambda': [1, 1.5],  \n",
    "    'class_weight': [None, 'balanced'] \n",
    "}\n",
    "# Define StratifiedKFold for cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define RandomizedSearchCV with stratified cross-validation and F1-score as the scoring metric\n",
    "random_search = RandomizedSearchCV(\n",
    "    lgbm_model,\n",
    "    param_distributions,\n",
    "    scoring=make_scorer(f1_score, average=\"binary\"),\n",
    "    cv=stratified_kfold,  # Use StratifiedKFold\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    n_iter=10,  # Number of parameter settings sampled\n",
    "    random_state=42,\n",
    "    error_score='raise'  # Set error_score to raise for debugging\n",
    ")\n",
    "\n",
    "# Measure training start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model on the resampled and scaled training data\n",
    "random_search.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Measure end time for training\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best parameters and retrain the model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = best_model.predict(X_val_scaled)\n",
    "y_val_pred_proba = best_model.predict_proba(X_val_scaled)[:, 1]  # For log loss and ROC-AUC\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_val, y_val_pred),\n",
    "    \"Precision\": precision_score(y_val, y_val_pred, average='binary'),  # Added average\n",
    "    \"Recall\": recall_score(y_val, y_val_pred, average='binary'),        # Added average\n",
    "    \"F1-Score\": f1_score(y_val, y_val_pred, average='binary'),          # Added average\n",
    "    \"Log Loss\": log_loss(y_val, y_val_pred_proba),                        # No average needed\n",
    "    \"Matthews Correlation Coefficient\": matthews_corrcoef(y_val, y_val_pred),  # No average needed\n",
    "    \"ROC-AUC\": roc_auc_score(y_val, y_val_pred_proba),                    # No average needed\n",
    "}\n",
    "\n",
    "# Print the evaluation metrics for validation set\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report for validation set\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and print training time\n",
    "train_time = end_time - start_time\n",
    "print(\"\\nTraining Time:\", round(train_time, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4eb7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
