{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46be54ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    GridSearchCV,\n",
    "    train_test_split\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    make_scorer\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e49178-79ff-4d59-a1f9-abb56ef14f2c",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9128851",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'telecom_data.csv')\n",
    "\n",
    "data.drop(['CustomerID'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a9e6c-b7a4-4e07-9ba2-3137397f06a5",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17832475-0304-4c85-a5d6-749912a21af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Monthly Engagement Ratio\n",
    "data['MonthlyEngagementRatio'] = data['ViewingHoursPerWeek'] / data['MonthlyCharges']\n",
    "\n",
    "# 2. Loyalty Indicator\n",
    "data['LoyaltyIndicator'] = pd.cut(data['AccountAge'], \n",
    "                                   bins=[0, 12, 36, float('inf')], \n",
    "                                   labels=['New', 'Mid-Level', 'Loyal'])\n",
    "\n",
    "# 3. Content Diversity\n",
    "data['ContentDiversity'] = (data['ContentType'] == 'Both').astype(int)\n",
    "\n",
    "# 4. Premium Cost Index\n",
    "average_cost = data.groupby('SubscriptionType')['MonthlyCharges'].transform('mean')\n",
    "data['PremiumCostIndex'] = data['MonthlyCharges'] > average_cost\n",
    "\n",
    "# 5. Device Engagement\n",
    "data['DeviceEngagement'] = ((data['DeviceRegistered'] != 'Unknown') & (data['MultiDeviceAccess'] == 'Yes')).astype(int)\n",
    "\n",
    "# 6. Support Need Intensity\n",
    "data['SupportNeedIntensity'] = data['SupportTicketsPerMonth'] / (data['AccountAge'] + 1)  # Avoid division by zero\n",
    "\n",
    "# 7. Churn Risk Indicators\n",
    "data['HighWatchlist'] = (data['WatchlistSize'] > 20).astype(int)\n",
    "\n",
    "# 8. FrequentDownloader\n",
    "data['FrequentDownloader'] = (data['ContentDownloadsPerMonth'] > data['ContentDownloadsPerMonth'].mean()).astype(int)\n",
    "\n",
    "# 9. Payment Stability\n",
    "stable_payment_methods = ['Bank transfer', 'Credit card']\n",
    "data['PaymentStability'] = data['PaymentMethod'].isin(stable_payment_methods).astype(int)\n",
    "\n",
    "# 10. Parental Control Impact\n",
    "data['ParentalControlImpact'] = ((data['ParentalControl'] == 'Yes') & (data['SubtitlesEnabled'] == 'Yes')).astype(int)\n",
    "\n",
    "# 11. Feature Interactions\n",
    "data['SubscriptionContentInteraction'] = data['SubscriptionType'] + \"_\" + data['ContentType']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc9765-31eb-44fb-8cd0-ffe17cb73664",
   "metadata": {},
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a632f82-1d68-4777-8c43-60b9d094a861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountAge</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>SubscriptionType</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>MultiDeviceAccess</th>\n",
       "      <th>DeviceRegistered</th>\n",
       "      <th>ViewingHoursPerWeek</th>\n",
       "      <th>AverageViewingDuration</th>\n",
       "      <th>ContentDownloadsPerMonth</th>\n",
       "      <th>...</th>\n",
       "      <th>Churn</th>\n",
       "      <th>MonthlyEngagementRatio</th>\n",
       "      <th>LoyaltyIndicator</th>\n",
       "      <th>ContentDiversity</th>\n",
       "      <th>PremiumCostIndex</th>\n",
       "      <th>SupportNeedIntensity</th>\n",
       "      <th>HighWatchlist</th>\n",
       "      <th>FrequentDownloader</th>\n",
       "      <th>PaymentStability</th>\n",
       "      <th>ParentalControlImpact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119</td>\n",
       "      <td>15.382382</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.563095</td>\n",
       "      <td>164.360194</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.856871</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119</td>\n",
       "      <td>16.104333</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13.422144</td>\n",
       "      <td>56.854608</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833449</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119</td>\n",
       "      <td>15.636604</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16.542962</td>\n",
       "      <td>40.246970</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.057964</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>119</td>\n",
       "      <td>7.005459</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.147423</td>\n",
       "      <td>113.853978</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.449281</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>119</td>\n",
       "      <td>14.713919</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31.802896</td>\n",
       "      <td>96.586233</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.161416</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AccountAge  MonthlyCharges  SubscriptionType  PaymentMethod  \\\n",
       "0         119       15.382382                 0              1   \n",
       "1         119       16.104333                 0              2   \n",
       "2         119       15.636604                 0              3   \n",
       "3         119        7.005459                 2              2   \n",
       "4         119       14.713919                 1              2   \n",
       "\n",
       "   PaperlessBilling  MultiDeviceAccess  DeviceRegistered  ViewingHoursPerWeek  \\\n",
       "0                 1                  1                 1            28.563095   \n",
       "1                 1                  1                 2            13.422144   \n",
       "2                 0                  1                 3            16.542962   \n",
       "3                 1                  0                 3             3.147423   \n",
       "4                 1                  0                 1            31.802896   \n",
       "\n",
       "   AverageViewingDuration  ContentDownloadsPerMonth  ...  Churn  \\\n",
       "0              164.360194                        43  ...      0   \n",
       "1               56.854608                        25  ...      0   \n",
       "2               40.246970                        25  ...      0   \n",
       "3              113.853978                        39  ...      0   \n",
       "4               96.586233                        41  ...      0   \n",
       "\n",
       "   MonthlyEngagementRatio  LoyaltyIndicator  ContentDiversity  \\\n",
       "0                1.856871                 0                 0   \n",
       "1                0.833449                 0                 0   \n",
       "2                1.057964                 0                 1   \n",
       "3                0.449281                 0                 0   \n",
       "4                2.161416                 0                 0   \n",
       "\n",
       "   PremiumCostIndex  SupportNeedIntensity  HighWatchlist  FrequentDownloader  \\\n",
       "0                 1              0.050000              1                   1   \n",
       "1                 1              0.000000              1                   1   \n",
       "2                 1              0.000000              0                   1   \n",
       "3                 0              0.008333              0                   1   \n",
       "4                 1              0.075000              1                   1   \n",
       "\n",
       "   PaymentStability  ParentalControlImpact  \n",
       "0                 1                      0  \n",
       "1                 0                      1  \n",
       "2                 0                      1  \n",
       "3                 0                      1  \n",
       "4                 0                      0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "data['SubscriptionType'] = le.fit_transform(data['SubscriptionType'])\n",
    "data['PaymentMethod'] = le.fit_transform(data['PaymentMethod'])\n",
    "data['PaperlessBilling'] = le.fit_transform(data['PaperlessBilling'])\n",
    "data['ContentType'] = le.fit_transform(data['ContentType'])\n",
    "data['MultiDeviceAccess'] = le.fit_transform(data['MultiDeviceAccess'])\n",
    "data['DeviceRegistered'] = le.fit_transform(data['DeviceRegistered'])\n",
    "data['GenrePreference'] = le.fit_transform(data['GenrePreference'])\n",
    "data['Gender'] = le.fit_transform(data['Gender'])\n",
    "data['ParentalControl'] = le.fit_transform(data['ParentalControl'])\n",
    "data['SubtitlesEnabled'] = le.fit_transform(data['SubtitlesEnabled'])\n",
    "data['LoyaltyIndicator'] = le.fit_transform(data['LoyaltyIndicator'])\n",
    "data['PremiumCostIndex'] = le.fit_transform(data['PremiumCostIndex'])\n",
    "data['SubscriptionContentInteraction'] = le.fit_transform(data['SubscriptionContentInteraction'])\n",
    "\n",
    "data.drop(['TotalCharges', 'ContentType', 'DeviceEngagement', 'SubscriptionContentInteraction'], axis = 1, inplace = True)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c1577-762c-416f-ac5c-08016ed01b96",
   "metadata": {},
   "source": [
    "# DATA SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "729b9152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train_val count: 195001\n",
      "data_test count: 48786\n"
     ]
    }
   ],
   "source": [
    "data_train_val = data[data['AccountAge']>24]\n",
    "data_test = data[data['AccountAge'] <=24]\n",
    "\n",
    "print('data_train_val count:', data_train_val['Churn'].count())\n",
    "print('data_test count:', data_test['Churn'].count())\n",
    "\n",
    "# Assume 'Target' is the name of the target column\n",
    "X = data_train_val.drop('Churn', axis=1)\n",
    "y = data_train_val['Churn']\n",
    "\n",
    "# Split the dataset into training (75%) and validation (25%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Apply Random Undersampling to the training data only\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb49c78-0066-41c5-a231-5faa82fafac5",
   "metadata": {},
   "source": [
    "# Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the resampled training data and the validation data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)  # Fit and transform on resampled training data\n",
    "X_val_scaled = scaler.transform(X_val)  # Only transform on the validation data, don't fit again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff8d82",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2e07798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 112 candidates, totalling 560 fits\n",
      "\n",
      "Validation Metrics:\n",
      "Accuracy: 0.6720\n",
      "Precision: 0.2726\n",
      "Recall: 0.6913\n",
      "F1-Score: 0.3910\n",
      "Log Loss: 0.6011\n",
      "Matthews Correlation Coefficient: 0.2656\n",
      "ROC-AUC: 0.7431\n",
      "\n",
      "Confusion Matrix (Validation):\n",
      "[[27628 13698]\n",
      " [ 2292  5133]]\n",
      "\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.67      0.78     41326\n",
      "           1       0.27      0.69      0.39      7425\n",
      "\n",
      "    accuracy                           0.67     48751\n",
      "   macro avg       0.60      0.68      0.58     48751\n",
      "weighted avg       0.82      0.67      0.72     48751\n",
      "\n",
      "\n",
      "Training Time: 69.83 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the logistic regression model with higher max_iter\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = [\n",
    "    {\n",
    "        'solver': ['liblinear'],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    {\n",
    "        'solver': ['lbfgs', 'newton-cg'],\n",
    "        'penalty': ['l2'],\n",
    "        'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    {\n",
    "        'solver': ['saga'],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'class_weight': ['balanced'],\n",
    "        'l1_ratio': [0.1, 0.5, 0.7, 1.0]  # Only for elasticnet\n",
    "    }\n",
    "]\n",
    "\n",
    "# Replace CV with Stratified Cross-Validation\n",
    "stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Replace RandomizedSearchCV with GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    log_reg,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average='binary'),\n",
    "    cv=stratified_cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    error_score='raise'\n",
    ")\n",
    "\n",
    "# Measure training start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model on the resampled and scaled training data\n",
    "grid_search.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Measure end time for training\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best parameters and retrain the model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Step 9: Make predictions on the validation set\n",
    "y_val_pred = best_model.predict(X_val_scaled)\n",
    "y_val_pred_proba = best_model.predict_proba(X_val_scaled)[:, 1]  # For log loss and ROC-AUC\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_val, y_val_pred),\n",
    "    \"Precision\": precision_score(y_val, y_val_pred, average='binary'),  \n",
    "    \"Recall\": recall_score(y_val, y_val_pred, average='binary'),        \n",
    "    \"F1-Score\": f1_score(y_val, y_val_pred, average='binary'),          \n",
    "    \"Log Loss\": log_loss(y_val, y_val_pred_proba),\n",
    "    \"Matthews Correlation Coefficient\": matthews_corrcoef(y_val, y_val_pred),\n",
    "    \"ROC-AUC\": roc_auc_score(y_val, y_val_pred_proba),\n",
    "}\n",
    "\n",
    "# Print the evaluation metrics for validation set\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report for validation set\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and print training time\n",
    "train_time = end_time - start_time\n",
    "print(\"\\nTraining Time:\", round(train_time, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be4e2f6-70e8-4e2a-b5bc-593fdb06b31c",
   "metadata": {},
   "source": [
    "# Feature Importance Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7556c8d-22a3-439b-a1ff-57cf01e482e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract feature importance for Logistic Regression\n",
    "feature_importance = np.abs(log_reg.coef_[0])  # Absolute values of coefficients\n",
    "feature_names = X.columns  # Feature names\n",
    "importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": feature_importance})\n",
    "\n",
    "#  Convert importance values to percentages\n",
    "importance_df[\"Importance (%)\"] = (importance_df[\"Importance\"] / importance_df[\"Importance\"].sum()) * 100\n",
    "\n",
    "#  Sort features by importance in descending order\n",
    "importance_df = importance_df.sort_values(by=\"Importance (%)\", ascending=False)\n",
    "\n",
    "#  Plot top feature importances with color gradient\n",
    "top_features = importance_df.head(15)\n",
    "colors = plt.cm.Blues(np.linspace(1, 0.4, len(top_features)))\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Increased the figure size\n",
    "bars = plt.barh(\n",
    "    top_features[\"Feature\"],\n",
    "    top_features[\"Importance (%)\"],\n",
    "    color=colors,\n",
    "    edgecolor=\"black\",\n",
    "    alpha=0.9,\n",
    ")\n",
    "\n",
    "plt.gca().invert_yaxis()  # Highest importance at the top\n",
    "plt.title(\"Feature Importances (Logistic Regression)\", fontsize=18)\n",
    "plt.xlabel(\"Importance (%)\", fontsize=14)\n",
    "\n",
    "# Add importance values on bars (rounded to 2 decimal places)\n",
    "for bar, value in zip(bars, top_features[\"Importance (%)\"]):\n",
    "    plt.text(\n",
    "        bar.get_width() + 0.001,  # Adjusted to provide more spacing\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{value:.1f}%\",  # Rounded to 2 decimal places\n",
    "        va=\"center\",\n",
    "        fontsize=12,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa8f867-3013-43de-8487-97cc8dee57f7",
   "metadata": {},
   "source": [
    "# Coefficients - Feature Importance Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99567041-7ac4-4967-84f2-8fedcf3abb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'log_reg' is your trained Logistic Regression model and 'X' is your feature DataFrame\n",
    "\n",
    "# Extract coefficients\n",
    "coefficients = log_reg.coef_[0]  # For binary classification, take the first row\n",
    "absolute_coefficients = np.abs(coefficients)  # Use absolute values for importance ranking\n",
    "\n",
    "# Pair coefficients with feature names\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": absolute_coefficients\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# (Optional) Convert to percentages\n",
    "feature_importance_df[\"Importance (%)\"] = (feature_importance_df[\"Importance\"] / feature_importance_df[\"Importance\"].sum()) * 100\n",
    "\n",
    "# Display the DataFrame with feature importance values\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6ccf7-44af-4992-b4ff-9f20f0a43360",
   "metadata": {},
   "source": [
    "# Permutation Importance - Feature Importance Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de1fff-07f2-4b10-95e6-3c8d8adc1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate Permutation Importance on the validation set using F1 score\n",
    "perm_importance = permutation_importance(\n",
    "    log_reg,  # Trained logistic regression model\n",
    "    X_val_scaled,  # Scaled validation features\n",
    "    y_val,  # Validation target labels\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    scoring=make_scorer(f1_score, average=\"binary\")  # Using F1 score as the scoring metric\n",
    ")\n",
    "\n",
    "# Extract feature importances and feature names\n",
    "feature_importances = perm_importance.importances_mean\n",
    "feature_names = X.columns\n",
    "\n",
    "# Convert importance values to percentages\n",
    "total_importance = feature_importances.sum()\n",
    "feature_importance_percentages = (feature_importances / total_importance) * 100\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance (%)\": feature_importance_percentages\n",
    "}).sort_values(by=\"Importance (%)\", ascending=False)\n",
    "\n",
    "# Display the feature importance DataFrame\n",
    "print(\"\\nPermutation Importance for Features (as Percentage):\")\n",
    "print(perm_importance_df)\n",
    "\n",
    "# Optionally save the results to an Excel file\n",
    "output_file = r\"permutation_importance_f1_score.xlsx\"\n",
    "perm_importance_df.to_excel(output_file, index=False)\n",
    "print(f\"\\nPermutation importance percentages have been saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a095a3a-4f19-485a-9c3f-458e61f5cebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e770ff96",
   "metadata": {},
   "source": [
    "# XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9148b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[0]\tvalidation_0-logloss:0.68744\n",
      "[1]\tvalidation_0-logloss:0.68379\n",
      "[2]\tvalidation_0-logloss:0.67896\n",
      "[3]\tvalidation_0-logloss:0.67438\n",
      "[4]\tvalidation_0-logloss:0.67078\n",
      "[5]\tvalidation_0-logloss:0.66681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\tvalidation_0-logloss:0.66327\n",
      "[7]\tvalidation_0-logloss:0.66016\n",
      "[8]\tvalidation_0-logloss:0.65751\n",
      "[9]\tvalidation_0-logloss:0.65515\n",
      "[10]\tvalidation_0-logloss:0.65244\n",
      "[11]\tvalidation_0-logloss:0.64981\n",
      "[12]\tvalidation_0-logloss:0.64753\n",
      "[13]\tvalidation_0-logloss:0.64528\n",
      "[14]\tvalidation_0-logloss:0.64317\n",
      "[15]\tvalidation_0-logloss:0.64124\n",
      "[16]\tvalidation_0-logloss:0.63971\n",
      "[17]\tvalidation_0-logloss:0.63795\n",
      "[18]\tvalidation_0-logloss:0.63634\n",
      "[19]\tvalidation_0-logloss:0.63503\n",
      "[20]\tvalidation_0-logloss:0.63339\n",
      "[21]\tvalidation_0-logloss:0.63217\n",
      "[22]\tvalidation_0-logloss:0.63101\n",
      "[23]\tvalidation_0-logloss:0.62984\n",
      "[24]\tvalidation_0-logloss:0.62866\n",
      "[25]\tvalidation_0-logloss:0.62773\n",
      "[26]\tvalidation_0-logloss:0.62661\n",
      "[27]\tvalidation_0-logloss:0.62542\n",
      "[28]\tvalidation_0-logloss:0.62448\n",
      "[29]\tvalidation_0-logloss:0.62361\n",
      "[30]\tvalidation_0-logloss:0.62276\n",
      "[31]\tvalidation_0-logloss:0.62198\n",
      "[32]\tvalidation_0-logloss:0.62113\n",
      "[33]\tvalidation_0-logloss:0.62041\n",
      "[34]\tvalidation_0-logloss:0.61988\n",
      "[35]\tvalidation_0-logloss:0.61899\n",
      "[36]\tvalidation_0-logloss:0.61845\n",
      "[37]\tvalidation_0-logloss:0.61783\n",
      "[38]\tvalidation_0-logloss:0.61727\n",
      "[39]\tvalidation_0-logloss:0.61684\n",
      "[40]\tvalidation_0-logloss:0.61643\n",
      "[41]\tvalidation_0-logloss:0.61600\n",
      "[42]\tvalidation_0-logloss:0.61562\n",
      "[43]\tvalidation_0-logloss:0.61525\n",
      "[44]\tvalidation_0-logloss:0.61468\n",
      "[45]\tvalidation_0-logloss:0.61421\n",
      "[46]\tvalidation_0-logloss:0.61378\n",
      "[47]\tvalidation_0-logloss:0.61344\n",
      "[48]\tvalidation_0-logloss:0.61314\n",
      "[49]\tvalidation_0-logloss:0.61287\n",
      "[50]\tvalidation_0-logloss:0.61257\n",
      "[51]\tvalidation_0-logloss:0.61224\n",
      "[52]\tvalidation_0-logloss:0.61171\n",
      "[53]\tvalidation_0-logloss:0.61143\n",
      "[54]\tvalidation_0-logloss:0.61111\n",
      "[55]\tvalidation_0-logloss:0.61092\n",
      "[56]\tvalidation_0-logloss:0.61069\n",
      "[57]\tvalidation_0-logloss:0.61041\n",
      "[58]\tvalidation_0-logloss:0.61026\n",
      "[59]\tvalidation_0-logloss:0.61007\n",
      "[60]\tvalidation_0-logloss:0.60975\n",
      "[61]\tvalidation_0-logloss:0.60965\n",
      "[62]\tvalidation_0-logloss:0.60936\n",
      "[63]\tvalidation_0-logloss:0.60915\n",
      "[64]\tvalidation_0-logloss:0.60904\n",
      "[65]\tvalidation_0-logloss:0.60884\n",
      "[66]\tvalidation_0-logloss:0.60869\n",
      "[67]\tvalidation_0-logloss:0.60845\n",
      "[68]\tvalidation_0-logloss:0.60828\n",
      "[69]\tvalidation_0-logloss:0.60806\n",
      "[70]\tvalidation_0-logloss:0.60786\n",
      "[71]\tvalidation_0-logloss:0.60772\n",
      "[72]\tvalidation_0-logloss:0.60767\n",
      "[73]\tvalidation_0-logloss:0.60740\n",
      "[74]\tvalidation_0-logloss:0.60710\n",
      "[75]\tvalidation_0-logloss:0.60698\n",
      "[76]\tvalidation_0-logloss:0.60684\n",
      "[77]\tvalidation_0-logloss:0.60660\n",
      "[78]\tvalidation_0-logloss:0.60660\n",
      "[79]\tvalidation_0-logloss:0.60649\n",
      "[80]\tvalidation_0-logloss:0.60636\n",
      "[81]\tvalidation_0-logloss:0.60620\n",
      "[82]\tvalidation_0-logloss:0.60600\n",
      "[83]\tvalidation_0-logloss:0.60583\n",
      "[84]\tvalidation_0-logloss:0.60576\n",
      "[85]\tvalidation_0-logloss:0.60554\n",
      "[86]\tvalidation_0-logloss:0.60529\n",
      "[87]\tvalidation_0-logloss:0.60511\n",
      "[88]\tvalidation_0-logloss:0.60499\n",
      "[89]\tvalidation_0-logloss:0.60502\n",
      "[90]\tvalidation_0-logloss:0.60483\n",
      "[91]\tvalidation_0-logloss:0.60477\n",
      "[92]\tvalidation_0-logloss:0.60467\n",
      "[93]\tvalidation_0-logloss:0.60470\n",
      "[94]\tvalidation_0-logloss:0.60460\n",
      "[95]\tvalidation_0-logloss:0.60449\n",
      "[96]\tvalidation_0-logloss:0.60434\n",
      "[97]\tvalidation_0-logloss:0.60417\n",
      "[98]\tvalidation_0-logloss:0.60406\n",
      "[99]\tvalidation_0-logloss:0.60385\n",
      "[100]\tvalidation_0-logloss:0.60385\n",
      "[101]\tvalidation_0-logloss:0.60379\n",
      "[102]\tvalidation_0-logloss:0.60370\n",
      "[103]\tvalidation_0-logloss:0.60354\n",
      "[104]\tvalidation_0-logloss:0.60348\n",
      "[105]\tvalidation_0-logloss:0.60335\n",
      "[106]\tvalidation_0-logloss:0.60326\n",
      "[107]\tvalidation_0-logloss:0.60330\n",
      "[108]\tvalidation_0-logloss:0.60321\n",
      "[109]\tvalidation_0-logloss:0.60315\n",
      "[110]\tvalidation_0-logloss:0.60312\n",
      "[111]\tvalidation_0-logloss:0.60318\n",
      "[112]\tvalidation_0-logloss:0.60309\n",
      "[113]\tvalidation_0-logloss:0.60312\n",
      "[114]\tvalidation_0-logloss:0.60319\n",
      "[115]\tvalidation_0-logloss:0.60313\n",
      "[116]\tvalidation_0-logloss:0.60301\n",
      "[117]\tvalidation_0-logloss:0.60293\n",
      "[118]\tvalidation_0-logloss:0.60284\n",
      "[119]\tvalidation_0-logloss:0.60276\n",
      "[120]\tvalidation_0-logloss:0.60271\n",
      "[121]\tvalidation_0-logloss:0.60270\n",
      "[122]\tvalidation_0-logloss:0.60258\n",
      "[123]\tvalidation_0-logloss:0.60261\n",
      "[124]\tvalidation_0-logloss:0.60252\n",
      "[125]\tvalidation_0-logloss:0.60259\n",
      "[126]\tvalidation_0-logloss:0.60268\n",
      "[127]\tvalidation_0-logloss:0.60257\n",
      "[128]\tvalidation_0-logloss:0.60244\n",
      "[129]\tvalidation_0-logloss:0.60223\n",
      "[130]\tvalidation_0-logloss:0.60228\n",
      "[131]\tvalidation_0-logloss:0.60231\n",
      "[132]\tvalidation_0-logloss:0.60222\n",
      "[133]\tvalidation_0-logloss:0.60221\n",
      "[134]\tvalidation_0-logloss:0.60210\n",
      "[135]\tvalidation_0-logloss:0.60216\n",
      "[136]\tvalidation_0-logloss:0.60216\n",
      "[137]\tvalidation_0-logloss:0.60215\n",
      "[138]\tvalidation_0-logloss:0.60222\n",
      "[139]\tvalidation_0-logloss:0.60221\n",
      "[140]\tvalidation_0-logloss:0.60211\n",
      "[141]\tvalidation_0-logloss:0.60208\n",
      "[142]\tvalidation_0-logloss:0.60209\n",
      "[143]\tvalidation_0-logloss:0.60217\n",
      "[144]\tvalidation_0-logloss:0.60213\n",
      "[145]\tvalidation_0-logloss:0.60211\n",
      "[146]\tvalidation_0-logloss:0.60210\n",
      "[147]\tvalidation_0-logloss:0.60200\n",
      "[148]\tvalidation_0-logloss:0.60196\n",
      "[149]\tvalidation_0-logloss:0.60183\n",
      "\n",
      "Validation Metrics:\n",
      "Accuracy: 0.6655\n",
      "Precision: 0.2704\n",
      "Recall: 0.7040\n",
      "F1-Score: 0.3907\n",
      "Log Loss: 0.6018\n",
      "Matthews Correlation Coefficient: 0.2663\n",
      "ROC-AUC: 0.7419\n",
      "\n",
      "Confusion Matrix (Validation):\n",
      "[[27219 14107]\n",
      " [ 2198  5227]]\n",
      "\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.66      0.77     41326\n",
      "           1       0.27      0.70      0.39      7425\n",
      "\n",
      "    accuracy                           0.67     48751\n",
      "   macro avg       0.60      0.68      0.58     48751\n",
      "weighted avg       0.83      0.67      0.71     48751\n",
      "\n",
      "\n",
      "Training Time: 67.77 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    use_label_encoder=False,  # To suppress warning in newer versions\n",
    "    eval_metric='logloss',  # Required by XGBoost\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150], \n",
    "    'learning_rate': [0.05, 0.1], \n",
    "    'max_depth': [4, 8],  \n",
    "    'min_child_weight': [1, 3],  \n",
    "    'subsample': [0.8], \n",
    "    'colsample_bytree': [0.8],  \n",
    "    'gamma': [0],  \n",
    "    'reg_alpha': [0], \n",
    "    'reg_lambda': [1]  \n",
    "}\n",
    "\n",
    "\n",
    "# Define GridSearchCV with F1-score as the scoring metric and StratifiedKFold for cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average='binary'),  # Binary F1-score for imbalanced datasets\n",
    "    cv=stratified_kfold,  # Use StratifiedKFold for cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    error_score='raise'  # Set error_score to raise for debugging\n",
    ")\n",
    "\n",
    "# Measure training start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model on the resampled and scaled training data\n",
    "grid_search.fit(\n",
    "    X_train_scaled, y_train_resampled,\n",
    "    eval_set=[(X_val_scaled, y_val)], \n",
    "    early_stopping_rounds=10,  # Stop if validation score doesn't improve for 10 rounds\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Measure end time for training\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best parameters and retrain the model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = best_model.predict(X_val_scaled)\n",
    "y_val_pred_proba = best_model.predict_proba(X_val_scaled)[:, 1]  # For log loss and ROC-AUC\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_val, y_val_pred),\n",
    "    \"Precision\": precision_score(y_val, y_val_pred, average='binary'),  \n",
    "    \"Recall\": recall_score(y_val, y_val_pred, average='binary'),        \n",
    "    \"F1-Score\": f1_score(y_val, y_val_pred, average='binary'),          \n",
    "    \"Log Loss\": log_loss(y_val, y_val_pred_proba),\n",
    "    \"Matthews Correlation Coefficient\": matthews_corrcoef(y_val, y_val_pred),\n",
    "    \"ROC-AUC\": roc_auc_score(y_val, y_val_pred_proba),\n",
    "}\n",
    "\n",
    "# Print the evaluation metrics for validation set\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report for validation set\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and print training time\n",
    "train_time = end_time - start_time\n",
    "print(\"\\nTraining Time:\", round(train_time, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473b238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eff48031",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d5a8660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "\n",
      "Validation Metrics:\n",
      "Accuracy: 0.6632\n",
      "Precision: 0.2672\n",
      "Recall: 0.6951\n",
      "F1-Score: 0.3860\n",
      "Log Loss: 0.6080\n",
      "Matthews Correlation Coefficient: 0.2590\n",
      "ROC-AUC: 0.7361\n",
      "\n",
      "Confusion Matrix (Validation):\n",
      "[[27170 14156]\n",
      " [ 2264  5161]]\n",
      "\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.66      0.77     41326\n",
      "           1       0.27      0.70      0.39      7425\n",
      "\n",
      "    accuracy                           0.66     48751\n",
      "   macro avg       0.60      0.68      0.58     48751\n",
      "weighted avg       0.82      0.66      0.71     48751\n",
      "\n",
      "\n",
      "Training Time: 1934.97 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the Random Forest classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 150],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt', 'log2'],  # Fixed deprecated 'auto'\n",
    "    'bootstrap': [True, False],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "# Define GridSearchCV with F1-score as the scoring metric and StratifiedKFold for cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf_model,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average=\"binary\"),  # Binary F1-score for imbalanced datasets\n",
    "    cv=stratified_kfold,  # Use StratifiedKFold for cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    error_score='raise'  # Set error_score to raise for debugging\n",
    ")\n",
    "\n",
    "# Measure training start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model on the resampled and scaled training data\n",
    "grid_search.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Measure end time for training\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best parameters and retrain the model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = best_model.predict(X_val_scaled)\n",
    "y_val_pred_proba = best_model.predict_proba(X_val_scaled)[:, 1]  # For log loss and ROC-AUC\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_val, y_val_pred),\n",
    "    \"Precision\": precision_score(y_val, y_val_pred, average=\"binary\"),  \n",
    "    \"Recall\": recall_score(y_val, y_val_pred, average=\"binary\"),        \n",
    "    \"F1-Score\": f1_score(y_val, y_val_pred, average=\"binary\"),          \n",
    "    \"Log Loss\": log_loss(y_val, y_val_pred_proba),\n",
    "    \"Matthews Correlation Coefficient\": matthews_corrcoef(y_val, y_val_pred),\n",
    "    \"ROC-AUC\": roc_auc_score(y_val, y_val_pred_proba)\n",
    "}\n",
    "\n",
    "# Print the evaluation metrics for validation set\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report for validation set\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and print training time\n",
    "train_time = end_time - start_time\n",
    "print(\"\\nTraining Time:\", round(train_time, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276dbf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22e16cea",
   "metadata": {},
   "source": [
    "# LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9463b9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 512 candidates, totalling 2560 fits\n",
      "[LightGBM] [Info] Number of positive: 22273, number of negative: 22273\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1768\n",
      "[LightGBM] [Info] Number of data points in the train set: 44546, number of used features: 26\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "Validation Metrics:\n",
      "Accuracy: 0.6642\n",
      "Precision: 0.2688\n",
      "Recall: 0.7007\n",
      "F1-Score: 0.3886\n",
      "Log Loss: 0.6032\n",
      "Matthews Correlation Coefficient: 0.2631\n",
      "ROC-AUC: 0.7402\n",
      "\n",
      "Confusion Matrix (Validation):\n",
      "[[27175 14151]\n",
      " [ 2222  5203]]\n",
      "\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.66      0.77     41326\n",
      "           1       0.27      0.70      0.39      7425\n",
      "\n",
      "    accuracy                           0.66     48751\n",
      "   macro avg       0.60      0.68      0.58     48751\n",
      "weighted avg       0.82      0.66      0.71     48751\n",
      "\n",
      "\n",
      "Training Time: 1190.41 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the LGBM classifier\n",
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [10, 20], \n",
    "    'num_leaves': [31, 40], \n",
    "    'min_child_samples': [10], \n",
    "    'subsample': [0.8, 1.0],  \n",
    "    'colsample_bytree': [0.8, 1.0], \n",
    "    'reg_alpha': [0, 0.1],  \n",
    "    'reg_lambda': [1, 1.5],  \n",
    "    'class_weight': [None, 'balanced'] \n",
    "}\n",
    "\n",
    "# Define StratifiedKFold for cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define GridSearchCV with stratified cross-validation and F1-score as the scoring metric\n",
    "grid_search = GridSearchCV(\n",
    "    lgbm_model,\n",
    "    param_grid,\n",
    "    scoring=make_scorer(f1_score, average=\"binary\"),  # Binary F1-score for imbalanced datasets\n",
    "    cv=stratified_kfold,  # Use StratifiedKFold\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    error_score='raise'  # Set error_score to raise for debugging\n",
    ")\n",
    "\n",
    "# Measure training start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the model on the resampled and scaled training data\n",
    "grid_search.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# Measure end time for training\n",
    "end_time = time.time()\n",
    "\n",
    "# Get the best parameters and retrain the model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = best_model.predict(X_val_scaled)\n",
    "y_val_pred_proba = best_model.predict_proba(X_val_scaled)[:, 1]  # For log loss and ROC-AUC\n",
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "val_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_val, y_val_pred),\n",
    "    \"Precision\": precision_score(y_val, y_val_pred),\n",
    "    \"Recall\": recall_score(y_val, y_val_pred),\n",
    "    \"F1-Score\": f1_score(y_val, y_val_pred),\n",
    "    \"Log Loss\": log_loss(y_val, y_val_pred_proba),\n",
    "    \"Matthews Correlation Coefficient\": matthews_corrcoef(y_val, y_val_pred),\n",
    "    \"ROC-AUC\": roc_auc_score(y_val, y_val_pred_proba),\n",
    "}\n",
    "\n",
    "# Print the evaluation metrics for validation set\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Confusion matrix and classification report for validation set\n",
    "print(\"\\nConfusion Matrix (Validation):\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Calculate and print training time\n",
    "train_time = end_time - start_time\n",
    "print(\"\\nTraining Time:\", round(train_time, 2), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d88a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
